{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "import LLDA.model.labeled_lda as llda\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "# sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0101', '0102', '0103', '0104', '0105', '0106', '0107', '0199', '0201', '0202', '0203', '0204', '0205', '0206', '0207', '0208', '0299', '0301', '0302', '0303', '0304', '0305', '0306', '0399', '0401', '0402', '0499', '0501', '0502', '0503', '0504', '0505', '0506', '0507', '0508', '0599', '0601', '0602', '0603', '0604', '0699']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('FRIS_projects_pubs.csv')\n",
    "dis_list = df['disciplines'].values\n",
    "topics = []\n",
    "for dis in dis_list:\n",
    "    dis = dis.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace(\" \", \"\").split(',')\n",
    "    dis = [d.strip() for d in dis]\n",
    "    topics.extend(dis)\n",
    "\n",
    "topics = sorted(list(set(topics)))\n",
    "print(topics)\n",
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "with open('FRIS_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "clean_data = {}\n",
    "for pro_id in data.keys():\n",
    "    if data[pro_id]['publications'] and data[pro_id]['disciplines']:\n",
    "        clean_data[pro_id] = data[pro_id]\n",
    "print(len(clean_data))\n",
    "\n",
    "pro_id_list = list(clean_data.keys())\n",
    "for pro_id in pro_id_list:\n",
    "    pubs = {}\n",
    "    pro_dis_list = []\n",
    "    for pub_id in clean_data[pro_id]['publications'].keys():\n",
    "        if clean_data[pro_id]['publications'][pub_id]['disciplines'] and clean_data[pro_id]['publications'][pub_id]['authors']:\n",
    "            pro_dis_list.extend(clean_data[pro_id]['publications'][pub_id]['disciplines'])\n",
    "            pubs[pub_id] = clean_data[pro_id]['publications'][pub_id]\n",
    "    \n",
    "    if pubs:\n",
    "        clean_data[pro_id]['publications'] = pubs\n",
    "        clean_data[pro_id]['disciplines'] = list(set(pro_dis_list))\n",
    "    else:\n",
    "        del clean_data[pro_id]\n",
    "\n",
    "print(len(clean_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json files\n",
    "with open('FRIS_projects_pubs_abstracts.json') as f:\n",
    "    abstract_data = json.load(f)\n",
    "\n",
    "with open('FRIS_projects_pubs_dis.json') as f:\n",
    "    dis_data = json.load(f)\n",
    "\n",
    "abstract_data_keys = [list(entry.keys())[0] for entry in abstract_data]\n",
    "project_data = []\n",
    "pub_data = []\n",
    "\n",
    "data = []\n",
    "for dis_entry in dis_data:\n",
    "    dis_entry = dict(dis_entry)\n",
    "    key = list(dis_entry.keys())[0]\n",
    "\n",
    "    abs_entry = None\n",
    "    for ent in abstract_data:\n",
    "        ent = dict(ent)\n",
    "        if key == list(ent.keys())[0]:\n",
    "            abs_entry = ent\n",
    "            break\n",
    "    \n",
    "    if not abs_entry:\n",
    "        continue\n",
    "\n",
    "    intersection = [x for x in abs_entry[key]['pub_ids'] if x in frozenset(dis_entry[key]['pub_ids'])]\n",
    "    if not intersection:\n",
    "        continue\n",
    "\n",
    "    project_abstract = abs_entry[key]['project_abstract']\n",
    "    project_dis = dis_entry[key]['project_disciplines']\n",
    "\n",
    "    abstract_idx = [abs_entry[key]['pub_ids'].index(x) for x in intersection]\n",
    "    pub_abstracts = [abs_entry[key]['pub_abstracts'][i] for i in abstract_idx]\n",
    "    dis_idx = [dis_entry[key]['pub_ids'].index(x) for x in intersection]\n",
    "    pub_dis = [dis_entry[key]['pub_disciplines'][i] for i in dis_idx]\n",
    "\n",
    "    data.append([key, project_abstract, project_dis, intersection, pub_abstracts, pub_dis])\n",
    "\n",
    "split = int(len(data) * 0.8)\n",
    "train_data = data[:split]\n",
    "test_data = data[split:]\n",
    "\n",
    "train_project_data = [(entry[1], entry[2]) for entry in train_data]\n",
    "train_pub_data = []\n",
    "for entry in train_data:\n",
    "    for i in range(len(entry[3])):\n",
    "        train_pub_data.append((entry[4][i], entry[5][i]))\n",
    "\n",
    "test_project_data = [(entry[1], entry[2]) for entry in test_data]\n",
    "test_pub_data = []\n",
    "for entry in test_data:\n",
    "    for i in range(len(entry[3])):\n",
    "        test_pub_data.append((entry[4][i], entry[5][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis = []\n",
    "for i in data:\n",
    "    dis.extend(i[2])\n",
    "pro_topic = set(dis)\n",
    "print(len(pro_topic))\n",
    "\n",
    "dis_list = []\n",
    "for i in data:\n",
    "    dis_list.extend(i[5])\n",
    "dis = []\n",
    "for i in dis_list:\n",
    "    dis.extend(i)\n",
    "pub_topic = set(dis)\n",
    "print(len(pub_topic))\n",
    "\n",
    "topic_intersection = pro_topic.intersection(pub_topic)\n",
    "len(topic_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = []\n",
    "for entry in data:\n",
    "    project_dis = entry[2]\n",
    "    pub_ids = entry[3]\n",
    "    pub_abstracts = entry[4]\n",
    "    pub_dis = entry[5]\n",
    "    pub_dis_new = []\n",
    "    pub_ids_new = []\n",
    "    for i, dis_list in enumerate(pub_dis):\n",
    "        if set(dis_list).issubset(topic_intersection):\n",
    "            pub_dis_new.append(dis_list)\n",
    "            pub_ids_new.append(entry[3][i])\n",
    "    \n",
    "    if not (set(project_dis).issubset(topic_intersection) and pub_dis_new):\n",
    "        continue\n",
    "\n",
    "    pub_idx = [pub_ids.index(x) for x in pub_ids_new]\n",
    "    pub_abstracts_new = [pub_abstracts[i] for i in pub_idx]\n",
    "    entry[3] = pub_ids_new\n",
    "    entry[4] = pub_abstracts_new\n",
    "    entry[5] = pub_dis_new\n",
    "\n",
    "    data_new.append(entry)\n",
    "\n",
    "split = int(len(data_new) * 0.8)\n",
    "train_data = data_new[:split]\n",
    "test_data = data_new[split:]\n",
    "\n",
    "train_project_data = [(entry[1], entry[2]) for entry in train_data]\n",
    "train_pub_data = []\n",
    "for entry in train_data:\n",
    "    for i in range(len(entry[3])):\n",
    "        train_pub_data.append((entry[4][i], entry[5][i]))\n",
    "\n",
    "test_project_data = [(entry[1], entry[2]) for entry in test_data]\n",
    "test_pub_data = []\n",
    "for entry in test_data:\n",
    "    for i in range(len(entry[3])):\n",
    "        test_pub_data.append((entry[4][i], entry[5][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis = []\n",
    "for i in data_new:\n",
    "    dis.extend(i[2])\n",
    "pro_topic = set(dis)\n",
    "print(len(pro_topic))\n",
    "\n",
    "dis_list = []\n",
    "for i in data_new:\n",
    "    dis_list.extend(i[5])\n",
    "dis = []\n",
    "for i in dis_list:\n",
    "    dis.extend(i)\n",
    "pub_topic = set(dis)\n",
    "print(len(pub_topic))\n",
    "\n",
    "topic_intersection = pro_topic.intersection(pub_topic)\n",
    "len(topic_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_new = []\n",
    "for entry in data_new:\n",
    "    project_dis = entry[2]\n",
    "    pub_ids = entry[3]\n",
    "    pub_abstracts = entry[4]\n",
    "    pub_dis = entry[5]\n",
    "    pub_dis_new = []\n",
    "    pub_ids_new = []\n",
    "    for i, dis_list in enumerate(pub_dis):\n",
    "        if set(dis_list).issubset(topic_intersection):\n",
    "            pub_dis_new.append(dis_list)\n",
    "            pub_ids_new.append(entry[3][i])\n",
    "    \n",
    "    if not (set(project_dis).issubset(topic_intersection) and pub_dis_new):\n",
    "        continue\n",
    "\n",
    "    pub_idx = [pub_ids.index(x) for x in pub_ids_new]\n",
    "    pub_abstracts_new = [pub_abstracts[i] for i in pub_idx]\n",
    "    entry[3] = pub_ids_new\n",
    "    entry[4] = pub_abstracts_new\n",
    "    entry[5] = pub_dis_new\n",
    "\n",
    "    data_new_new.append(entry)\n",
    "\n",
    "split = int(len(data_new_new) * 0.95)\n",
    "train_data = data_new_new[:split]\n",
    "test_data = data_new_new[split:]\n",
    "\n",
    "train_project_data = [(entry[1], entry[2]) for entry in train_data]\n",
    "train_pub_data = []\n",
    "for entry in train_data:\n",
    "    for i in range(len(entry[3])):\n",
    "        train_pub_data.append((entry[4][i], entry[5][i]))\n",
    "\n",
    "test_project_data = [(entry[1], entry[2]) for entry in test_data]\n",
    "test_pub_data = []\n",
    "for entry in test_data:\n",
    "    for i in range(len(entry[3])):\n",
    "        test_pub_data.append((entry[4][i], entry[5][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis = []\n",
    "for i in data_new_new:\n",
    "    dis.extend(i[2])\n",
    "pro_topic = set(dis)\n",
    "print(len(pro_topic))\n",
    "\n",
    "dis_list = []\n",
    "for i in data_new_new:\n",
    "    dis_list.extend(i[5])\n",
    "dis = []\n",
    "for i in dis_list:\n",
    "    dis.extend(i)\n",
    "pub_topic = set(dis)\n",
    "print(len(pub_topic))\n",
    "\n",
    "topic_intersection = pro_topic.intersection(pub_topic)\n",
    "len(topic_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train project data: ('Sensorineural hearing loss (SNHL) affects one out of three older than 65, yet imaging methods have only recently become sensitive enough to isolate the specific elements which constitute SNHL (e.g. synaptic and hair-cell damage). Because these elements have different functional roles for hearing, their damage needs to be individually characterized and taken into account in therapeutic interventions. Differential diagnosis is possible in controlled animal experiments via post-mortem histology, but is much harder to achieve in humans where unknown mixtures of SNHL need to be quantified non-invasively.\\nTo translate findings from the animal domain to humans, we adopt a computational model of the auditory periphery which integrates the histopathology and physiology of hearing to predict how human physiological responses are affected by SNHL. We focus on simultaneously modeling the neural generators of auditory evoked potentials, otoacoustic emissions and the middle-ear-muscle-reflex. A single framework which connects the generators of all three physiological responses does not exist but forms the scientific breakthrough opportunity, as data from several animal studies on SNHL can be combined to identify and study candidate physiological markers for human diagnostics. The resulting model will be used to (i) simulate and test how single SNHL profiles affect physiological responses differently, and (ii), study how SNHL affects sound processing along the ascending auditory pathway.', ['0306', '0206', '0301'])\n",
      "Train pub data: ('Aging, noise exposure, and ototoxic medications lead to cochlear synapse loss in animal models. As cochlear function is highly conserved across mammalian species, synaptopathy likely occurs in humans as well. Synaptopathy is predicted to result in perceptual deficits including tinnitus, hyperacusis, and difficulty understanding speech-in-noise. The lack of a method for diagnosing synaptopathy in living humans hinders studies designed to determine if noise-induced synaptopathy occurs in humans, identify the perceptual consequences of synaptopathy, or test potential drug treatments. Several physiological measures are sensitive to synaptopathy in animal models includ-ing auditory brainstem response (ABR) wave I amplitude. However, it is unclear how to translate these measures to synaptopathy diagnosis in humans. This work demonstrates how a human computational model of the auditory periphery, which can predict ABR waveforms and distortion product otoacoustic emissions (DPOAEs), can be used to predict synaptic loss in individual human participants based on their measured DPOAE levels and ABR wave I amplitudes. Lower predicted synapse numbers were associated with advancing age, higher noise exposure history, increased likelihood of tinnitus, and poorer speech-in-noise perception. These findings demonstrate the utility of this modeling approach in predicting synapse counts from physiological data in individual human subjects.', ['0103'])\n"
     ]
    }
   ],
   "source": [
    "print('Train project data:', train_project_data[0])\n",
    "print('Train pub data:', train_pub_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n",
      "146 513\n",
      "8 64\n"
     ]
    }
   ],
   "source": [
    "print(len(data_new_new))\n",
    "print(len(train_project_data), len(train_pub_data))\n",
    "print(len(test_project_data), len(test_pub_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load from disk\n",
    "# llda_model_project = llda.LldaModel()\n",
    "# llda_model_project.load_model_from_dir(\"LLDA/data_models/pro_abstracts_disciplines\", load_derivative_properties=False)\n",
    "# print(\"llda_model_project\", llda_model_project)\n",
    "# # print(\"Top-5 terms of topic 'virus': \", llda_model_project.top_terms_of_topic(\"virus\", 5, False))\n",
    "# print(\"Doc-Topic Matrix: \\n\", llda_model_project.theta)\n",
    "# print(\"Topic-Term Matrix: \\n\", llda_model_project.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_project_copy = train_project_data.copy()\n",
    "train_pub_copy = train_pub_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeled-LDA Model:\n",
      "\tK = 32\n",
      "\tM = 146\n",
      "\tT = 7084\n",
      "\tWN = 27783\n",
      "\tLN = 253\n",
      "\talpha = 1.5625\n",
      "\teta = 0.001\n",
      "\tperplexity = 626.3098551831756\n",
      "\t\n",
      "\n",
      "Labeled-LDA Model:\n",
      "\tK = 32\n",
      "\tM = 513\n",
      "\tT = 17841\n",
      "\tWN = 102055\n",
      "\tLN = 1147\n",
      "\talpha = 1.5625\n",
      "\teta = 0.001\n",
      "\tperplexity = 1199.36248292791\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "llda_model_project = llda.LldaModel(labeled_documents=train_project_copy)\n",
    "print(llda_model_project)\n",
    "llda_model_pub = llda.LldaModel(labeled_documents=train_pub_copy)\n",
    "print(llda_model_pub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration: 0, perplexity: 626.3098551831756\n",
      "gibbs sample count:  27783\n",
      "after iteration: 1, perplexity: 537.9745709990104\n",
      "gibbs sample count:  27783\n",
      "after iteration: 2, perplexity: 530.3084066511476\n",
      "gibbs sample count:  27783\n",
      "after iteration: 3, perplexity: 533.5084783041957\n",
      "gibbs sample count:  27783\n",
      "after iteration: 4, perplexity: 534.1111164759184\n",
      "gibbs sample count:  27783\n",
      "after iteration: 5, perplexity: 535.6866421315981\n",
      "gibbs sample count:  27783\n",
      "after iteration: 6, perplexity: 537.6771979380878\n",
      "gibbs sample count:  27783\n",
      "after iteration: 7, perplexity: 538.0678367162153\n",
      "gibbs sample count:  27783\n",
      "after iteration: 8, perplexity: 537.3626278738654\n",
      "gibbs sample count:  27783\n",
      "after iteration: 9, perplexity: 539.5771113976285\n",
      "gibbs sample count:  27783\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "llda_model_project.training(iteration=10, log=True)\n",
    "# while True:\n",
    "#     print(\"iteration %s sampling...\" % (llda_model.iteration + 1))\n",
    "#     llda_model.training(1)\n",
    "#     print(\"after iteration: %s, perplexity: %s\" % (llda_model.iteration, llda_model.perplexity()))\n",
    "#     print(\"delta beta: %s\" % llda_model.delta_beta)\n",
    "#     if llda_model.is_convergent(method=\"beta\", delta=0.01):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration: 0, perplexity: 1199.36248292791\n",
      "gibbs sample count:  102055\n",
      "after iteration: 1, perplexity: 1087.5740881499812\n",
      "gibbs sample count:  102055\n",
      "after iteration: 2, perplexity: 1067.8215730019954\n",
      "gibbs sample count:  102055\n",
      "after iteration: 3, perplexity: 1064.3029035619495\n",
      "gibbs sample count:  102055\n",
      "after iteration: 4, perplexity: 1062.0042117761184\n",
      "gibbs sample count:  102055\n",
      "after iteration: 5, perplexity: 1059.4585316258572\n",
      "gibbs sample count:  102055\n",
      "after iteration: 6, perplexity: 1059.196811247672\n",
      "gibbs sample count:  102055\n",
      "after iteration: 7, perplexity: 1057.7283022565457\n",
      "gibbs sample count:  102055\n",
      "after iteration: 8, perplexity: 1057.4111038365454\n",
      "gibbs sample count:  102055\n",
      "after iteration: 9, perplexity: 1057.275269868792\n",
      "gibbs sample count:  102055\n"
     ]
    }
   ],
   "source": [
    "llda_model_pub.training(iteration=10, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # update\n",
    "# print(\"before updating: \", llda_model)\n",
    "# update_labeled_documents = [(\"new example test example test example test example test\", [\"example\", \"test\"])]\n",
    "# llda_model.update(labeled_documents=update_labeled_documents)\n",
    "# print(\"after updating: \", llda_model)\n",
    "\n",
    "# # train again\n",
    "# # llda_model.training(iteration=10, log=True)\n",
    "# while True:\n",
    "#     print(\"iteration %s sampling...\" % (llda_model.iteration + 1))\n",
    "#     llda_model.training(1)\n",
    "#     print(\"after iteration: %s, perplexity: %s\" % (llda_model.iteration, llda_model.perplexity()))\n",
    "#     print(\"delta beta: %s\" % llda_model.delta_beta)\n",
    "#     if llda_model.is_convergent(method=\"beta\", delta=0.01):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer models have become an important tool to understand causal mechanisms underlying human movement. Simulating realistic musculoskeletal dynamics is critical to understanding neural control of muscle activity and motor performance. State-of-the-art simulations of motion are based on phenomenological muscle-tendon models that do not take into account the history dependence of muscle action. However, force enhancement and depression following dynamic contractions measured in isolated muscle fibers may be as high as 50% of the corresponding steady-state isometric force. The aim of the proposed project is therefore to investigate the contribution of history dependent muscle action to dynamic whole-body human movements by combining experimental data and computational modeling. We hypothesize that history-dependent muscle action contributes substantially to stability of walking in the presence of external perturbations and increases performance of sports movements such as jumping and running.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('0101', 0.010812883435582822),\n",
       " ('0102', 0.012039877300613497),\n",
       " ('0103', 0.022469325153374235),\n",
       " ('0104', 0.017561349693251533),\n",
       " ('0105', 0.05375766871165644),\n",
       " ('0106', 0.03412576687116564),\n",
       " ('0107', 0.010199386503067485),\n",
       " ('0201', 0.02492331288343558),\n",
       " ('0202', 0.010812883435582822),\n",
       " ('0203', 0.02062883435582822),\n",
       " ('0204', 0.01142638036809816),\n",
       " ('0205', 0.0169478527607362),\n",
       " ('0206', 0.010812883435582822),\n",
       " ('0207', 0.010199386503067485),\n",
       " ('0208', 0.023696319018404906),\n",
       " ('0299', 0.02492331288343558),\n",
       " ('0301', 0.027377300613496934),\n",
       " ('0302', 0.020015337423312886),\n",
       " ('0303', 0.022469325153374235),\n",
       " ('0304', 0.012039877300613497),\n",
       " ('0305', 0.017561349693251533),\n",
       " ('0306', 0.019401840490797545),\n",
       " ('0401', 0.02553680981595092),\n",
       " ('0402', 0.009585889570552147),\n",
       " ('0501', 0.02799079754601227),\n",
       " ('0502', 0.010812883435582822),\n",
       " ('0504', 0.009585889570552147),\n",
       " ('0507', 0.012039877300613497),\n",
       " ('0602', 0.010812883435582822),\n",
       " ('0603', 0.0169478527607362),\n",
       " ('0604', 0.010812883435582822),\n",
       " ('common_topic', 0.4316717791411043)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference\n",
    "# note: the result topics may be different for difference training, because gibbs sampling is a random algorithm\n",
    "document = test_project_data[0][0]\n",
    "print(document)\n",
    "\n",
    "topics = llda_model_project.inference(document=document, iteration=100, times=10)\n",
    "# print sorted topics\n",
    "sorted(topics, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00958589, 0.01388037, 0.02369632, 0.01940184, 0.02308282,\n",
       "        0.03167178, 0.01081288, 0.0279908 , 0.01203988, 0.01878834,\n",
       "        0.01203988, 0.01572086, 0.01019939, 0.01142638, 0.02001534,\n",
       "        0.01817485, 0.03535276, 0.02369632, 0.01633436, 0.01081288,\n",
       "        0.01572086, 0.05130368, 0.01633436, 0.01203988, 0.02308282,\n",
       "        0.01019939, 0.01081288, 0.01326687, 0.01081288, 0.01388037,\n",
       "        0.01019939, 0.4476227 ],\n",
       "       [0.0117515 , 0.01115269, 0.02013473, 0.01414671, 0.01474551,\n",
       "        0.0117515 , 0.00995509, 0.02372754, 0.02432635, 0.02133234,\n",
       "        0.01115269, 0.0129491 , 0.00935629, 0.00995509, 0.00935629,\n",
       "        0.01474551, 0.03929641, 0.0380988 , 0.03510479, 0.02612275,\n",
       "        0.01893713, 0.02851796, 0.0129491 , 0.05007485, 0.01654192,\n",
       "        0.00995509, 0.01773952, 0.01474551, 0.01115269, 0.00995509,\n",
       "        0.02372754, 0.41654192],\n",
       "       [0.00737028, 0.00878538, 0.01727594, 0.01916274, 0.00737028,\n",
       "        0.01774764, 0.01114387, 0.01208726, 0.06680425, 0.00784198,\n",
       "        0.00878538, 0.01114387, 0.01255896, 0.00737028, 0.01869104,\n",
       "        0.00784198, 0.03520047, 0.01633255, 0.01067217, 0.01680425,\n",
       "        0.00737028, 0.01303066, 0.00784198, 0.09038915, 0.02152123,\n",
       "        0.00831368, 0.00878538, 0.01020047, 0.00737028, 0.01161557,\n",
       "        0.00972877, 0.48284198],\n",
       "       [0.00814315, 0.01436722, 0.02349585, 0.01934647, 0.0064834 ,\n",
       "        0.02100622, 0.00731328, 0.01312241, 0.04133817, 0.01602697,\n",
       "        0.01187759, 0.01602697, 0.00731328, 0.00855809, 0.02930498,\n",
       "        0.01395228, 0.01727178, 0.01187759, 0.01727178, 0.0098029 ,\n",
       "        0.00772822, 0.01644191, 0.00897303, 0.00855809, 0.02598548,\n",
       "        0.01644191, 0.01312241, 0.01436722, 0.00689834, 0.00897303,\n",
       "        0.00772822, 0.55088174],\n",
       "       [0.01201613, 0.02427419, 0.02685484, 0.02879032, 0.01008065,\n",
       "        0.03137097, 0.01330645, 0.01846774, 0.01975806, 0.02491935,\n",
       "        0.01395161, 0.01395161, 0.01201613, 0.01459677, 0.01524194,\n",
       "        0.03653226, 0.02233871, 0.01846774, 0.02040323, 0.01653226,\n",
       "        0.01137097, 0.01395161, 0.01395161, 0.01137097, 0.01330645,\n",
       "        0.03072581, 0.02620968, 0.01459677, 0.02040323, 0.02491935,\n",
       "        0.01072581, 0.41459677],\n",
       "       [0.00947581, 0.02022849, 0.02022849, 0.01700269, 0.00840054,\n",
       "        0.03528226, 0.01485215, 0.02721774, 0.03420699, 0.01001344,\n",
       "        0.01377688, 0.01323925, 0.00840054, 0.01055108, 0.01055108,\n",
       "        0.01646505, 0.10194892, 0.01592742, 0.01001344, 0.00840054,\n",
       "        0.01861559, 0.01915323, 0.02076613, 0.01001344, 0.03958333,\n",
       "        0.01162634, 0.02291667, 0.01108871, 0.00840054, 0.01055108,\n",
       "        0.00947581, 0.41162634],\n",
       "       [0.01175459, 0.02597477, 0.0149656 , 0.01175459, 0.00716743,\n",
       "        0.06037844, 0.00946101, 0.01037844, 0.02230505, 0.00716743,\n",
       "        0.00808486, 0.00716743, 0.01267202, 0.00900229, 0.00762615,\n",
       "        0.0122133 , 0.06313073, 0.02826835, 0.02597477, 0.00854358,\n",
       "        0.01037844, 0.04478211, 0.00808486, 0.0122133 , 0.01037844,\n",
       "        0.00854358, 0.01313073, 0.00991972, 0.01083716, 0.00808486,\n",
       "        0.00716743, 0.49248853],\n",
       "       [0.00945431, 0.01402284, 0.02214467, 0.01706853, 0.01503807,\n",
       "        0.04955584, 0.01199239, 0.01148477, 0.01199239, 0.0089467 ,\n",
       "        0.00793147, 0.00843909, 0.01148477, 0.02722081, 0.00843909,\n",
       "        0.01046954, 0.08458122, 0.03940355, 0.01046954, 0.0089467 ,\n",
       "        0.0160533 , 0.03280457, 0.01148477, 0.0089467 , 0.01046954,\n",
       "        0.0089467 , 0.0196066 , 0.00996193, 0.00996193, 0.00996193,\n",
       "        0.01402284, 0.45869289]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = test_project_data\n",
    "topic_matrix_project = np.zeros((len(documents), llda_model_project.K))\n",
    "\n",
    "for i, (document, _) in enumerate(documents):\n",
    "    topics = llda_model_project.inference(document=document, iteration=100, times=10)\n",
    "    sorted_topics = sorted(topics, key=lambda x: x[0])\n",
    "    for j, (topic, prob) in enumerate(sorted_topics):\n",
    "        topic_matrix_project[i, j] = prob\n",
    "\n",
    "topic_matrix_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00785   , 0.00825   , 0.00865   , ..., 0.01185   , 0.01585   ,\n",
       "        0.49385   ],\n",
       "       [0.02000689, 0.01008953, 0.0089876 , ..., 0.00568182, 0.00457989,\n",
       "        0.3952135 ],\n",
       "       [0.00858696, 0.00597826, 0.01293478, ..., 0.00945652, 0.0198913 ,\n",
       "        0.49496377],\n",
       "       ...,\n",
       "       [0.01127049, 0.0101776 , 0.0364071 , ..., 0.03203552, 0.11181694,\n",
       "        0.34296448],\n",
       "       [0.01578664, 0.00975216, 0.01794181, ..., 0.00802802, 0.00845905,\n",
       "        0.40070043],\n",
       "       [0.02516026, 0.02003205, 0.03285256, ..., 0.02003205, 0.02644231,\n",
       "        0.05849359]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = test_pub_data\n",
    "topic_matrix_pub = np.zeros((len(documents), llda_model_pub.K))\n",
    "\n",
    "for i, (document, _) in enumerate(documents):\n",
    "    topics = llda_model_pub.inference(document=document, iteration=100, times=10)\n",
    "    sorted_topics = sorted(topics, key=lambda x: x[0])\n",
    "    for j, (topic, prob) in enumerate(sorted_topics):\n",
    "        topic_matrix_pub[i, j] = prob\n",
    "\n",
    "topic_matrix_pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity on test data: 851.9515738134049\n",
      "perplexity on training data: 540.1862195263787\n"
     ]
    }
   ],
   "source": [
    "# perplexity\n",
    "# calculate perplexity on test data\n",
    "perplexity = llda_model_project.perplexity(documents=map(lambda x: x[0], test_project_data),\n",
    "                                   iteration=30,\n",
    "                                   times=10)\n",
    "print(\"perplexity on test data: %s\" % perplexity)\n",
    "# calculate perplexity on training data\n",
    "print(\"perplexity on training data: %s\" % llda_model_project.perplexity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity on test data: 1459.1165750361565\n",
      "perplexity on training data: 1056.7377561158273\n"
     ]
    }
   ],
   "source": [
    "# perplexity\n",
    "# calculate perplexity on test data\n",
    "perplexity = llda_model_pub.perplexity(documents=map(lambda x: x[0], test_pub_data),\n",
    "                                   iteration=30,\n",
    "                                   times=10)\n",
    "print(\"perplexity on test data: %s\" % perplexity)\n",
    "# calculate perplexity on training data\n",
    "print(\"perplexity on training data: %s\" % llda_model_pub.perplexity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "save_model_dir = \"LLDA/data_models/topic_pro_disciplines\"\n",
    "# llda_model.save_model_to_dir(save_model_dir, save_derivative_properties=True)\n",
    "llda_model_project.save_model_to_dir(save_model_dir)\n",
    "# save to disk\n",
    "save_model_dir = \"LLDA/data_models/topic_pub_disciplines\"\n",
    "# llda_model.save_model_to_dir(save_model_dir, save_derivative_properties=True)\n",
    "llda_model_pub.save_model_to_dir(save_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2123314748621048"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jensenshannon(topic_matrix_project[0], topic_matrix_pub[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = np.zeros((len(topic_matrix_project), len(topic_matrix_pub)))\n",
    "\n",
    "for i in range(len(topic_matrix_project)):\n",
    "    for j in range(len(topic_matrix_pub)):\n",
    "        distance_matrix[i, j] = jensenshannon(topic_matrix_project[i], topic_matrix_pub[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.212331</td>\n",
       "      <td>0.269175</td>\n",
       "      <td>0.267569</td>\n",
       "      <td>0.445697</td>\n",
       "      <td>0.295521</td>\n",
       "      <td>0.201339</td>\n",
       "      <td>0.223425</td>\n",
       "      <td>0.424491</td>\n",
       "      <td>0.376280</td>\n",
       "      <td>0.244440</td>\n",
       "      <td>0.170431</td>\n",
       "      <td>0.318744</td>\n",
       "      <td>0.227287</td>\n",
       "      <td>0.291724</td>\n",
       "      <td>0.303605</td>\n",
       "      <td>0.174852</td>\n",
       "      <td>0.196058</td>\n",
       "      <td>0.357871</td>\n",
       "      <td>0.212024</td>\n",
       "      <td>0.223233</td>\n",
       "      <td>0.260043</td>\n",
       "      <td>0.283683</td>\n",
       "      <td>0.291394</td>\n",
       "      <td>0.238238</td>\n",
       "      <td>0.161269</td>\n",
       "      <td>0.270053</td>\n",
       "      <td>0.212523</td>\n",
       "      <td>0.279449</td>\n",
       "      <td>0.230363</td>\n",
       "      <td>0.246146</td>\n",
       "      <td>0.194323</td>\n",
       "      <td>0.267119</td>\n",
       "      <td>0.205868</td>\n",
       "      <td>0.203038</td>\n",
       "      <td>0.180197</td>\n",
       "      <td>0.191759</td>\n",
       "      <td>0.221595</td>\n",
       "      <td>0.343878</td>\n",
       "      <td>0.272553</td>\n",
       "      <td>0.295849</td>\n",
       "      <td>0.215019</td>\n",
       "      <td>0.247162</td>\n",
       "      <td>0.257373</td>\n",
       "      <td>0.186615</td>\n",
       "      <td>0.231500</td>\n",
       "      <td>0.240552</td>\n",
       "      <td>0.198433</td>\n",
       "      <td>0.282268</td>\n",
       "      <td>0.254371</td>\n",
       "      <td>0.181228</td>\n",
       "      <td>0.288842</td>\n",
       "      <td>0.182244</td>\n",
       "      <td>0.192192</td>\n",
       "      <td>0.163516</td>\n",
       "      <td>0.305225</td>\n",
       "      <td>0.203716</td>\n",
       "      <td>0.223856</td>\n",
       "      <td>0.192887</td>\n",
       "      <td>0.201121</td>\n",
       "      <td>0.187965</td>\n",
       "      <td>0.266233</td>\n",
       "      <td>0.200574</td>\n",
       "      <td>0.140742</td>\n",
       "      <td>0.356169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.231136</td>\n",
       "      <td>0.298907</td>\n",
       "      <td>0.262725</td>\n",
       "      <td>0.432929</td>\n",
       "      <td>0.308049</td>\n",
       "      <td>0.216204</td>\n",
       "      <td>0.197784</td>\n",
       "      <td>0.415110</td>\n",
       "      <td>0.391351</td>\n",
       "      <td>0.240240</td>\n",
       "      <td>0.188680</td>\n",
       "      <td>0.320688</td>\n",
       "      <td>0.243810</td>\n",
       "      <td>0.285619</td>\n",
       "      <td>0.269250</td>\n",
       "      <td>0.184387</td>\n",
       "      <td>0.208006</td>\n",
       "      <td>0.350881</td>\n",
       "      <td>0.191898</td>\n",
       "      <td>0.233026</td>\n",
       "      <td>0.255561</td>\n",
       "      <td>0.295676</td>\n",
       "      <td>0.249022</td>\n",
       "      <td>0.209673</td>\n",
       "      <td>0.132081</td>\n",
       "      <td>0.256331</td>\n",
       "      <td>0.216692</td>\n",
       "      <td>0.256276</td>\n",
       "      <td>0.195544</td>\n",
       "      <td>0.229222</td>\n",
       "      <td>0.216700</td>\n",
       "      <td>0.230112</td>\n",
       "      <td>0.180935</td>\n",
       "      <td>0.190366</td>\n",
       "      <td>0.175983</td>\n",
       "      <td>0.176993</td>\n",
       "      <td>0.182422</td>\n",
       "      <td>0.318054</td>\n",
       "      <td>0.269011</td>\n",
       "      <td>0.274085</td>\n",
       "      <td>0.196611</td>\n",
       "      <td>0.244016</td>\n",
       "      <td>0.244425</td>\n",
       "      <td>0.198950</td>\n",
       "      <td>0.239333</td>\n",
       "      <td>0.262293</td>\n",
       "      <td>0.218032</td>\n",
       "      <td>0.256241</td>\n",
       "      <td>0.219834</td>\n",
       "      <td>0.214002</td>\n",
       "      <td>0.264511</td>\n",
       "      <td>0.233603</td>\n",
       "      <td>0.223005</td>\n",
       "      <td>0.178004</td>\n",
       "      <td>0.285513</td>\n",
       "      <td>0.223407</td>\n",
       "      <td>0.216813</td>\n",
       "      <td>0.229262</td>\n",
       "      <td>0.237625</td>\n",
       "      <td>0.196559</td>\n",
       "      <td>0.254240</td>\n",
       "      <td>0.200791</td>\n",
       "      <td>0.199554</td>\n",
       "      <td>0.352770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.262093</td>\n",
       "      <td>0.345770</td>\n",
       "      <td>0.297904</td>\n",
       "      <td>0.459758</td>\n",
       "      <td>0.338451</td>\n",
       "      <td>0.264828</td>\n",
       "      <td>0.300378</td>\n",
       "      <td>0.445076</td>\n",
       "      <td>0.406316</td>\n",
       "      <td>0.314303</td>\n",
       "      <td>0.221573</td>\n",
       "      <td>0.378324</td>\n",
       "      <td>0.315817</td>\n",
       "      <td>0.328758</td>\n",
       "      <td>0.359592</td>\n",
       "      <td>0.234488</td>\n",
       "      <td>0.281701</td>\n",
       "      <td>0.382778</td>\n",
       "      <td>0.284271</td>\n",
       "      <td>0.288223</td>\n",
       "      <td>0.317382</td>\n",
       "      <td>0.361321</td>\n",
       "      <td>0.334612</td>\n",
       "      <td>0.308704</td>\n",
       "      <td>0.225072</td>\n",
       "      <td>0.336785</td>\n",
       "      <td>0.314870</td>\n",
       "      <td>0.352075</td>\n",
       "      <td>0.296104</td>\n",
       "      <td>0.318435</td>\n",
       "      <td>0.312355</td>\n",
       "      <td>0.345496</td>\n",
       "      <td>0.252117</td>\n",
       "      <td>0.276095</td>\n",
       "      <td>0.240099</td>\n",
       "      <td>0.275102</td>\n",
       "      <td>0.280114</td>\n",
       "      <td>0.406421</td>\n",
       "      <td>0.337973</td>\n",
       "      <td>0.366031</td>\n",
       "      <td>0.292938</td>\n",
       "      <td>0.308544</td>\n",
       "      <td>0.319961</td>\n",
       "      <td>0.258929</td>\n",
       "      <td>0.323347</td>\n",
       "      <td>0.309847</td>\n",
       "      <td>0.281418</td>\n",
       "      <td>0.335834</td>\n",
       "      <td>0.302107</td>\n",
       "      <td>0.262799</td>\n",
       "      <td>0.369576</td>\n",
       "      <td>0.282436</td>\n",
       "      <td>0.280703</td>\n",
       "      <td>0.215390</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.285541</td>\n",
       "      <td>0.291374</td>\n",
       "      <td>0.271476</td>\n",
       "      <td>0.284450</td>\n",
       "      <td>0.233440</td>\n",
       "      <td>0.335295</td>\n",
       "      <td>0.264292</td>\n",
       "      <td>0.259811</td>\n",
       "      <td>0.431062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.194030</td>\n",
       "      <td>0.303330</td>\n",
       "      <td>0.266287</td>\n",
       "      <td>0.440226</td>\n",
       "      <td>0.305398</td>\n",
       "      <td>0.201898</td>\n",
       "      <td>0.270895</td>\n",
       "      <td>0.414908</td>\n",
       "      <td>0.391861</td>\n",
       "      <td>0.283398</td>\n",
       "      <td>0.153417</td>\n",
       "      <td>0.357203</td>\n",
       "      <td>0.283488</td>\n",
       "      <td>0.296868</td>\n",
       "      <td>0.341593</td>\n",
       "      <td>0.160076</td>\n",
       "      <td>0.236565</td>\n",
       "      <td>0.358383</td>\n",
       "      <td>0.264940</td>\n",
       "      <td>0.246965</td>\n",
       "      <td>0.281548</td>\n",
       "      <td>0.326550</td>\n",
       "      <td>0.308143</td>\n",
       "      <td>0.293788</td>\n",
       "      <td>0.205594</td>\n",
       "      <td>0.310957</td>\n",
       "      <td>0.296530</td>\n",
       "      <td>0.339363</td>\n",
       "      <td>0.288350</td>\n",
       "      <td>0.295935</td>\n",
       "      <td>0.274034</td>\n",
       "      <td>0.320125</td>\n",
       "      <td>0.235447</td>\n",
       "      <td>0.245809</td>\n",
       "      <td>0.205408</td>\n",
       "      <td>0.240509</td>\n",
       "      <td>0.267282</td>\n",
       "      <td>0.397039</td>\n",
       "      <td>0.331096</td>\n",
       "      <td>0.357198</td>\n",
       "      <td>0.285530</td>\n",
       "      <td>0.280128</td>\n",
       "      <td>0.285723</td>\n",
       "      <td>0.204507</td>\n",
       "      <td>0.313710</td>\n",
       "      <td>0.265971</td>\n",
       "      <td>0.253067</td>\n",
       "      <td>0.324404</td>\n",
       "      <td>0.274675</td>\n",
       "      <td>0.191375</td>\n",
       "      <td>0.365994</td>\n",
       "      <td>0.214107</td>\n",
       "      <td>0.256395</td>\n",
       "      <td>0.140820</td>\n",
       "      <td>0.275247</td>\n",
       "      <td>0.245669</td>\n",
       "      <td>0.250852</td>\n",
       "      <td>0.234797</td>\n",
       "      <td>0.235064</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>0.329234</td>\n",
       "      <td>0.246028</td>\n",
       "      <td>0.225202</td>\n",
       "      <td>0.439002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.192167</td>\n",
       "      <td>0.284199</td>\n",
       "      <td>0.263752</td>\n",
       "      <td>0.423042</td>\n",
       "      <td>0.266588</td>\n",
       "      <td>0.204078</td>\n",
       "      <td>0.226005</td>\n",
       "      <td>0.401792</td>\n",
       "      <td>0.363075</td>\n",
       "      <td>0.257674</td>\n",
       "      <td>0.173032</td>\n",
       "      <td>0.319419</td>\n",
       "      <td>0.266170</td>\n",
       "      <td>0.263199</td>\n",
       "      <td>0.294908</td>\n",
       "      <td>0.185194</td>\n",
       "      <td>0.184006</td>\n",
       "      <td>0.316555</td>\n",
       "      <td>0.215976</td>\n",
       "      <td>0.212271</td>\n",
       "      <td>0.230081</td>\n",
       "      <td>0.259868</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.215968</td>\n",
       "      <td>0.183735</td>\n",
       "      <td>0.256267</td>\n",
       "      <td>0.214428</td>\n",
       "      <td>0.294768</td>\n",
       "      <td>0.240020</td>\n",
       "      <td>0.242160</td>\n",
       "      <td>0.264770</td>\n",
       "      <td>0.292781</td>\n",
       "      <td>0.221711</td>\n",
       "      <td>0.196552</td>\n",
       "      <td>0.178453</td>\n",
       "      <td>0.209296</td>\n",
       "      <td>0.235989</td>\n",
       "      <td>0.331807</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>0.298760</td>\n",
       "      <td>0.205708</td>\n",
       "      <td>0.251692</td>\n",
       "      <td>0.237645</td>\n",
       "      <td>0.189748</td>\n",
       "      <td>0.296389</td>\n",
       "      <td>0.239090</td>\n",
       "      <td>0.250531</td>\n",
       "      <td>0.306992</td>\n",
       "      <td>0.254307</td>\n",
       "      <td>0.193334</td>\n",
       "      <td>0.328848</td>\n",
       "      <td>0.177173</td>\n",
       "      <td>0.246042</td>\n",
       "      <td>0.171403</td>\n",
       "      <td>0.306080</td>\n",
       "      <td>0.181027</td>\n",
       "      <td>0.210450</td>\n",
       "      <td>0.190480</td>\n",
       "      <td>0.195803</td>\n",
       "      <td>0.198598</td>\n",
       "      <td>0.313906</td>\n",
       "      <td>0.207109</td>\n",
       "      <td>0.194303</td>\n",
       "      <td>0.353991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.227421</td>\n",
       "      <td>0.330559</td>\n",
       "      <td>0.291175</td>\n",
       "      <td>0.440144</td>\n",
       "      <td>0.320245</td>\n",
       "      <td>0.215985</td>\n",
       "      <td>0.264471</td>\n",
       "      <td>0.423508</td>\n",
       "      <td>0.392776</td>\n",
       "      <td>0.285099</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>0.361661</td>\n",
       "      <td>0.291661</td>\n",
       "      <td>0.302138</td>\n",
       "      <td>0.323407</td>\n",
       "      <td>0.231093</td>\n",
       "      <td>0.223998</td>\n",
       "      <td>0.357871</td>\n",
       "      <td>0.190520</td>\n",
       "      <td>0.222842</td>\n",
       "      <td>0.285719</td>\n",
       "      <td>0.320884</td>\n",
       "      <td>0.288319</td>\n",
       "      <td>0.236260</td>\n",
       "      <td>0.207287</td>\n",
       "      <td>0.297178</td>\n",
       "      <td>0.230077</td>\n",
       "      <td>0.315725</td>\n",
       "      <td>0.236395</td>\n",
       "      <td>0.293558</td>\n",
       "      <td>0.274675</td>\n",
       "      <td>0.315549</td>\n",
       "      <td>0.187002</td>\n",
       "      <td>0.236686</td>\n",
       "      <td>0.221168</td>\n",
       "      <td>0.221981</td>\n",
       "      <td>0.255075</td>\n",
       "      <td>0.354142</td>\n",
       "      <td>0.300190</td>\n",
       "      <td>0.309966</td>\n",
       "      <td>0.261817</td>\n",
       "      <td>0.250147</td>\n",
       "      <td>0.287619</td>\n",
       "      <td>0.218483</td>\n",
       "      <td>0.288996</td>\n",
       "      <td>0.263976</td>\n",
       "      <td>0.229589</td>\n",
       "      <td>0.286892</td>\n",
       "      <td>0.281306</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.320949</td>\n",
       "      <td>0.232185</td>\n",
       "      <td>0.160740</td>\n",
       "      <td>0.194159</td>\n",
       "      <td>0.347870</td>\n",
       "      <td>0.231698</td>\n",
       "      <td>0.259296</td>\n",
       "      <td>0.225539</td>\n",
       "      <td>0.258421</td>\n",
       "      <td>0.214001</td>\n",
       "      <td>0.264383</td>\n",
       "      <td>0.244474</td>\n",
       "      <td>0.184209</td>\n",
       "      <td>0.363463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.237743</td>\n",
       "      <td>0.318926</td>\n",
       "      <td>0.261430</td>\n",
       "      <td>0.450897</td>\n",
       "      <td>0.307259</td>\n",
       "      <td>0.216096</td>\n",
       "      <td>0.251892</td>\n",
       "      <td>0.425919</td>\n",
       "      <td>0.374492</td>\n",
       "      <td>0.284840</td>\n",
       "      <td>0.199962</td>\n",
       "      <td>0.340818</td>\n",
       "      <td>0.262285</td>\n",
       "      <td>0.301872</td>\n",
       "      <td>0.322134</td>\n",
       "      <td>0.192754</td>\n",
       "      <td>0.224051</td>\n",
       "      <td>0.364510</td>\n",
       "      <td>0.234410</td>\n",
       "      <td>0.235309</td>\n",
       "      <td>0.270007</td>\n",
       "      <td>0.328255</td>\n",
       "      <td>0.296902</td>\n",
       "      <td>0.245436</td>\n",
       "      <td>0.186654</td>\n",
       "      <td>0.286576</td>\n",
       "      <td>0.269328</td>\n",
       "      <td>0.304774</td>\n",
       "      <td>0.266121</td>\n",
       "      <td>0.274235</td>\n",
       "      <td>0.214434</td>\n",
       "      <td>0.289099</td>\n",
       "      <td>0.179917</td>\n",
       "      <td>0.211176</td>\n",
       "      <td>0.220942</td>\n",
       "      <td>0.215163</td>\n",
       "      <td>0.257677</td>\n",
       "      <td>0.348508</td>\n",
       "      <td>0.304606</td>\n",
       "      <td>0.297665</td>\n",
       "      <td>0.270615</td>\n",
       "      <td>0.240137</td>\n",
       "      <td>0.257925</td>\n",
       "      <td>0.228871</td>\n",
       "      <td>0.256765</td>\n",
       "      <td>0.264680</td>\n",
       "      <td>0.205660</td>\n",
       "      <td>0.293140</td>\n",
       "      <td>0.273694</td>\n",
       "      <td>0.218135</td>\n",
       "      <td>0.318117</td>\n",
       "      <td>0.220829</td>\n",
       "      <td>0.220292</td>\n",
       "      <td>0.192762</td>\n",
       "      <td>0.303687</td>\n",
       "      <td>0.260829</td>\n",
       "      <td>0.273911</td>\n",
       "      <td>0.204799</td>\n",
       "      <td>0.228974</td>\n",
       "      <td>0.207565</td>\n",
       "      <td>0.259622</td>\n",
       "      <td>0.245294</td>\n",
       "      <td>0.152925</td>\n",
       "      <td>0.403281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.225723</td>\n",
       "      <td>0.315418</td>\n",
       "      <td>0.260917</td>\n",
       "      <td>0.437747</td>\n",
       "      <td>0.326298</td>\n",
       "      <td>0.207609</td>\n",
       "      <td>0.239020</td>\n",
       "      <td>0.420471</td>\n",
       "      <td>0.377875</td>\n",
       "      <td>0.259226</td>\n",
       "      <td>0.204978</td>\n",
       "      <td>0.325375</td>\n",
       "      <td>0.250672</td>\n",
       "      <td>0.293732</td>\n",
       "      <td>0.299412</td>\n",
       "      <td>0.204049</td>\n",
       "      <td>0.225889</td>\n",
       "      <td>0.357126</td>\n",
       "      <td>0.203677</td>\n",
       "      <td>0.241615</td>\n",
       "      <td>0.260744</td>\n",
       "      <td>0.314206</td>\n",
       "      <td>0.281374</td>\n",
       "      <td>0.230568</td>\n",
       "      <td>0.173574</td>\n",
       "      <td>0.287648</td>\n",
       "      <td>0.246615</td>\n",
       "      <td>0.304046</td>\n",
       "      <td>0.241789</td>\n",
       "      <td>0.276622</td>\n",
       "      <td>0.228383</td>\n",
       "      <td>0.270963</td>\n",
       "      <td>0.189780</td>\n",
       "      <td>0.225087</td>\n",
       "      <td>0.204307</td>\n",
       "      <td>0.197421</td>\n",
       "      <td>0.246856</td>\n",
       "      <td>0.352074</td>\n",
       "      <td>0.290315</td>\n",
       "      <td>0.295054</td>\n",
       "      <td>0.260389</td>\n",
       "      <td>0.239846</td>\n",
       "      <td>0.267117</td>\n",
       "      <td>0.213869</td>\n",
       "      <td>0.233112</td>\n",
       "      <td>0.270514</td>\n",
       "      <td>0.190944</td>\n",
       "      <td>0.268288</td>\n",
       "      <td>0.265796</td>\n",
       "      <td>0.210032</td>\n",
       "      <td>0.323690</td>\n",
       "      <td>0.231517</td>\n",
       "      <td>0.197046</td>\n",
       "      <td>0.196551</td>\n",
       "      <td>0.339294</td>\n",
       "      <td>0.255477</td>\n",
       "      <td>0.265939</td>\n",
       "      <td>0.196166</td>\n",
       "      <td>0.232390</td>\n",
       "      <td>0.209879</td>\n",
       "      <td>0.218709</td>\n",
       "      <td>0.218815</td>\n",
       "      <td>0.156619</td>\n",
       "      <td>0.373238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.212331  0.269175  0.267569  0.445697  0.295521  0.201339  0.223425   \n",
       "1  0.231136  0.298907  0.262725  0.432929  0.308049  0.216204  0.197784   \n",
       "2  0.262093  0.345770  0.297904  0.459758  0.338451  0.264828  0.300378   \n",
       "3  0.194030  0.303330  0.266287  0.440226  0.305398  0.201898  0.270895   \n",
       "4  0.192167  0.284199  0.263752  0.423042  0.266588  0.204078  0.226005   \n",
       "5  0.227421  0.330559  0.291175  0.440144  0.320245  0.215985  0.264471   \n",
       "6  0.237743  0.318926  0.261430  0.450897  0.307259  0.216096  0.251892   \n",
       "7  0.225723  0.315418  0.260917  0.437747  0.326298  0.207609  0.239020   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.424491  0.376280  0.244440  0.170431  0.318744  0.227287  0.291724   \n",
       "1  0.415110  0.391351  0.240240  0.188680  0.320688  0.243810  0.285619   \n",
       "2  0.445076  0.406316  0.314303  0.221573  0.378324  0.315817  0.328758   \n",
       "3  0.414908  0.391861  0.283398  0.153417  0.357203  0.283488  0.296868   \n",
       "4  0.401792  0.363075  0.257674  0.173032  0.319419  0.266170  0.263199   \n",
       "5  0.423508  0.392776  0.285099  0.206900  0.361661  0.291661  0.302138   \n",
       "6  0.425919  0.374492  0.284840  0.199962  0.340818  0.262285  0.301872   \n",
       "7  0.420471  0.377875  0.259226  0.204978  0.325375  0.250672  0.293732   \n",
       "\n",
       "         14        15        16        17        18        19        20  \\\n",
       "0  0.303605  0.174852  0.196058  0.357871  0.212024  0.223233  0.260043   \n",
       "1  0.269250  0.184387  0.208006  0.350881  0.191898  0.233026  0.255561   \n",
       "2  0.359592  0.234488  0.281701  0.382778  0.284271  0.288223  0.317382   \n",
       "3  0.341593  0.160076  0.236565  0.358383  0.264940  0.246965  0.281548   \n",
       "4  0.294908  0.185194  0.184006  0.316555  0.215976  0.212271  0.230081   \n",
       "5  0.323407  0.231093  0.223998  0.357871  0.190520  0.222842  0.285719   \n",
       "6  0.322134  0.192754  0.224051  0.364510  0.234410  0.235309  0.270007   \n",
       "7  0.299412  0.204049  0.225889  0.357126  0.203677  0.241615  0.260744   \n",
       "\n",
       "         21        22        23        24        25        26        27  \\\n",
       "0  0.283683  0.291394  0.238238  0.161269  0.270053  0.212523  0.279449   \n",
       "1  0.295676  0.249022  0.209673  0.132081  0.256331  0.216692  0.256276   \n",
       "2  0.361321  0.334612  0.308704  0.225072  0.336785  0.314870  0.352075   \n",
       "3  0.326550  0.308143  0.293788  0.205594  0.310957  0.296530  0.339363   \n",
       "4  0.259868  0.270700  0.215968  0.183735  0.256267  0.214428  0.294768   \n",
       "5  0.320884  0.288319  0.236260  0.207287  0.297178  0.230077  0.315725   \n",
       "6  0.328255  0.296902  0.245436  0.186654  0.286576  0.269328  0.304774   \n",
       "7  0.314206  0.281374  0.230568  0.173574  0.287648  0.246615  0.304046   \n",
       "\n",
       "         28        29        30        31        32        33        34  \\\n",
       "0  0.230363  0.246146  0.194323  0.267119  0.205868  0.203038  0.180197   \n",
       "1  0.195544  0.229222  0.216700  0.230112  0.180935  0.190366  0.175983   \n",
       "2  0.296104  0.318435  0.312355  0.345496  0.252117  0.276095  0.240099   \n",
       "3  0.288350  0.295935  0.274034  0.320125  0.235447  0.245809  0.205408   \n",
       "4  0.240020  0.242160  0.264770  0.292781  0.221711  0.196552  0.178453   \n",
       "5  0.236395  0.293558  0.274675  0.315549  0.187002  0.236686  0.221168   \n",
       "6  0.266121  0.274235  0.214434  0.289099  0.179917  0.211176  0.220942   \n",
       "7  0.241789  0.276622  0.228383  0.270963  0.189780  0.225087  0.204307   \n",
       "\n",
       "         35        36        37        38        39        40        41  \\\n",
       "0  0.191759  0.221595  0.343878  0.272553  0.295849  0.215019  0.247162   \n",
       "1  0.176993  0.182422  0.318054  0.269011  0.274085  0.196611  0.244016   \n",
       "2  0.275102  0.280114  0.406421  0.337973  0.366031  0.292938  0.308544   \n",
       "3  0.240509  0.267282  0.397039  0.331096  0.357198  0.285530  0.280128   \n",
       "4  0.209296  0.235989  0.331807  0.267607  0.298760  0.205708  0.251692   \n",
       "5  0.221981  0.255075  0.354142  0.300190  0.309966  0.261817  0.250147   \n",
       "6  0.215163  0.257677  0.348508  0.304606  0.297665  0.270615  0.240137   \n",
       "7  0.197421  0.246856  0.352074  0.290315  0.295054  0.260389  0.239846   \n",
       "\n",
       "         42        43        44        45        46        47        48  \\\n",
       "0  0.257373  0.186615  0.231500  0.240552  0.198433  0.282268  0.254371   \n",
       "1  0.244425  0.198950  0.239333  0.262293  0.218032  0.256241  0.219834   \n",
       "2  0.319961  0.258929  0.323347  0.309847  0.281418  0.335834  0.302107   \n",
       "3  0.285723  0.204507  0.313710  0.265971  0.253067  0.324404  0.274675   \n",
       "4  0.237645  0.189748  0.296389  0.239090  0.250531  0.306992  0.254307   \n",
       "5  0.287619  0.218483  0.288996  0.263976  0.229589  0.286892  0.281306   \n",
       "6  0.257925  0.228871  0.256765  0.264680  0.205660  0.293140  0.273694   \n",
       "7  0.267117  0.213869  0.233112  0.270514  0.190944  0.268288  0.265796   \n",
       "\n",
       "         49        50        51        52        53        54        55  \\\n",
       "0  0.181228  0.288842  0.182244  0.192192  0.163516  0.305225  0.203716   \n",
       "1  0.214002  0.264511  0.233603  0.223005  0.178004  0.285513  0.223407   \n",
       "2  0.262799  0.369576  0.282436  0.280703  0.215390  0.329897  0.285541   \n",
       "3  0.191375  0.365994  0.214107  0.256395  0.140820  0.275247  0.245669   \n",
       "4  0.193334  0.328848  0.177173  0.246042  0.171403  0.306080  0.181027   \n",
       "5  0.216667  0.320949  0.232185  0.160740  0.194159  0.347870  0.231698   \n",
       "6  0.218135  0.318117  0.220829  0.220292  0.192762  0.303687  0.260829   \n",
       "7  0.210032  0.323690  0.231517  0.197046  0.196551  0.339294  0.255477   \n",
       "\n",
       "         56        57        58        59        60        61        62  \\\n",
       "0  0.223856  0.192887  0.201121  0.187965  0.266233  0.200574  0.140742   \n",
       "1  0.216813  0.229262  0.237625  0.196559  0.254240  0.200791  0.199554   \n",
       "2  0.291374  0.271476  0.284450  0.233440  0.335295  0.264292  0.259811   \n",
       "3  0.250852  0.234797  0.235064  0.194030  0.329234  0.246028  0.225202   \n",
       "4  0.210450  0.190480  0.195803  0.198598  0.313906  0.207109  0.194303   \n",
       "5  0.259296  0.225539  0.258421  0.214001  0.264383  0.244474  0.184209   \n",
       "6  0.273911  0.204799  0.228974  0.207565  0.259622  0.245294  0.152925   \n",
       "7  0.265939  0.196166  0.232390  0.209879  0.218709  0.218815  0.156619   \n",
       "\n",
       "         63  \n",
       "0  0.356169  \n",
       "1  0.352770  \n",
       "2  0.431062  \n",
       "3  0.439002  \n",
       "4  0.353991  \n",
       "5  0.363463  \n",
       "6  0.403281  \n",
       "7  0.373238  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.DataFrame(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
