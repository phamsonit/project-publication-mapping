{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "import LLDA.model.labeled_lda as llda\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the data\n",
    "# data = pd.read_csv('FRIS_projects_pubs.csv')\n",
    "# data.head()\n",
    "\n",
    "# discipline_list = []\n",
    "# for i in range(len(data)):\n",
    "#     temp = data['disciplines'][i].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(',')\n",
    "#     temp = [x.strip() for x in temp]\n",
    "#     discipline_list += temp\n",
    "\n",
    "# discipline_list = list(set(discipline_list))\n",
    "\n",
    "# print(sorted(discipline_list))\n",
    "# print(len(discipline_list))\n",
    "\n",
    "# # initialize data\n",
    "# all_documents = []\n",
    "# for i in range(len(data)):\n",
    "#     temp = data['disciplines'][i].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(',')\n",
    "#     temp = [x.strip() for x in temp]\n",
    "#     all_documents.append((data['title'][i], temp))\n",
    "\n",
    "# split = int(len(all_documents)*0.8)\n",
    "# labeled_documents, documents = all_documents[:split], all_documents[split:]\n",
    "# labeled_documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The relevance of the study of Latin & Greek (L&G) in secondary education is being questioned more and more. Proponents have claimed that it greatly improves linguistic abilities and even cognition beyond the linguistic domain, e.g. analytical thinking. Numerous American scholars have set up field tests, but they focused on the influence of Latin on English vocabulary acquisition by native speakers. Moreover, these studies all lack a theoretical framework for the supposed effects of L&G, so that the premises underlying hypotheses remain implicit. As a result, the debate on the educational value of L&G is still predominantly based on intuitive and ideologically-driven arguments rather than on empirical research. Therefore we propose a longitudinal study on the effects of L&G on Flemish pupils’ language competence (near transfer) and cognition (far transfer). Latin & Greek differ from the other languages standardly offered in Flemish secondary education because they are no longer spoken, but also because they are more complex. As such, the common didactic approach pays much more attention to grammatical parsing. We will investigate whether this intensive training in grammatical analysis results in cognitive transfer, which will be dissociated from a priori pupil characteristics. In addition to the main study, the features of L&G learning suspected to cause transfer will be simulated in a short-term artificial grammar learning experiment.',\n",
       "  ['Predictive models of academic achievement (AA) are used in high-stakes applications (e.g., selection procedures for higher education). Considering the far-reaching consequences of their outcomes, these models should show as little bias as possible. While numerous studies have researched the impact of gender on isolated predictors of AA, this has not been done in the context of such models. As such, we explored the role of gender in program-specific prediction models of AA. Data was examined from 5,016 first-year students across 16 programs in open access higher education. Results revealed interactions between gender and several predictors of AA. Furthermore, while the models exhibited little difference in the accuracy of their predictions for male and female students, using gender-specific models substantially improved our predictions. We also found that male and female models of AA differ greatly in terms of the predictors included in their composition, irrespective of the study program’s gender balance.']),\n",
       " ('Viruses have evolved to hijack key cellular components of their natural host. The VirEOS project will analyze how medically relevant viruses such as human respiratory syncytial, hepatitis E, yellow fever, Zika or Kaposi sarcoma-associated herpes virus, interfere with RNA sensing and RNA homeostasis in their host cells. The impact of identified factors on the pathogenicity and on the immune responses will then be analyzed in vivo, using infectious models. Understanding how viruses manipulate cellular RNA should not only provide new targets to the development of antiviral drugs but also help to identify important cellular hubs in RNA physiology.',\n",
       "  ['Protein mutagenesis is essential for unveiling the molecular mechanisms underlying protein function in health, disease, and evolution. In the past decade, deep mutational scanning methods have evolved to support the functional analysis of nearly all possible single-amino acid changes in a protein of interest. While historically these methods were developed in lower organisms such as E. coli and yeast, recent technological advancements have resulted in the increased use of mammalian cells, particularly for studying proteins involved in human disease. These advancements will aid significantly in the classification and interpretation of variants of unknown significance, which are being discovered at large scale due to the current surge in the use of whole-genome sequencing in clinical contexts. Here, we explore the experimental aspects of deep mutational scanning studies in mammalian cells and report the different methods used in each step of the workflow, ultimately providing a useful guide toward the design of such studies.']),\n",
       " ('This project offers the first comprehensive reconstruction and interpretation of receptions of ancient novels (1st-4th cent. AD) in (Greek, Arabic and western vernacular) secular narrative from Late Antiquity and the early Middle Ages. Novel Echoes follows up from the ERC Starting Grant project Novel Saints (on hagiography). It does so by taking ancient novelistic receptions towards entirely new, unexplored horizons. Our knowledge about the early history of the novel is incomplete. Receptions of ancient novels have been studied for periods from the 11th and 12th cent. onwards but not systematically examined for preceding eras –much to the detriment of the study of both narrative (then and later) and the history of fiction. This project pursues the hypothesis that different secular, narrative traditions in this period were impacted (directly or indirectly) by ancient novelistic influences of different kinds and adopted (and adapted) them to various degrees and purposes; and that, since the ancient novel is a genre defined by its own fictionality, its reception in later narrative impacts notions of truth and authentication in ways that other (often more authoritative) literary models (e.g. Homer and the Bible) do not. Novel Echoes strikes a balance between breath and depth by envisaging three objectives: \\n 1. the creation of a reference tool charting all types of novelistic influence in secular narrative from the 4th to the 12th cent.; \\n 2. the in-depth study of particular sets of texts and the analysis of their implicit conceptualizations of truth, authentication, fiction and narrative; \\n 3. the reconstruction of routes of transmission in both the West and the East. Given the project’ innovative focus, it will enhance our understanding of both the corpus texts and the early history of the novel; place the study of corpus texts on an improved methodological footing; and contribute to the theoretical study of the much-vexed question of how to conceptualize fiction.',\n",
       "  ['Letters are famously easy to recognise, notoriously hard to define. Both real and fictitious letters can look identical to the point that there are no formal criteria which can distinguish one from the other. This has long been a point of anxiety in scholarship which has considered the value of an ancient letter to be determined by its authenticity, necessitating a strict binary opposition of genuine as opposed to fake letters. This volume challenges this dichotomy directly. Rather than defining epistolary fiction as a literary genre in opposition to ‘genuine’ letters or reducing it down to fixed rhetorical features, it argues that fiction is an inherent and fluid property of letters which ancient writers recognised and exploited. This volume contributes to wider scholarship on ancient fiction by demonstrating through the multiplicity of genres, contexts, and time periods discussed how complex and multifaceted ancient awareness of fictionality was. As such, this volume shows that letters are uniquely well-placed to unsettle disciplinary boundaries of fact and fiction, authentic and spurious, and that this allows for a deeper understanding of how ancient writers conceptualised and manipulated the fictional potential of letters.',\n",
       "   'The Letters ascribed to Euripides have long been recognised to be inauthentic, and have been characterised variously as a simple forgery, a creative response to the wider biographical tradition surrounding the tragedian, or as some kind of epistolary novel. Yet, the wider consequences of these different labels have not yet been explored. This chapter considers how the Euripidean Letters consistently thematise the tensions inherent in their own pseudepigraphic conceit and in doing so challenge any assumptions scholars may have about fictional letters as a familiar or naturalised form of epistolary writing in antiquity. By exploring how this collection aligns itself within and against biographical traditions about Euripides while simultaneously undercutting their own authenticity, this chapter uses the Letters to nuance the terminology of fictional letters and to interrogate the boundaries of epistolary fiction as a genre.']),\n",
       " (\"The ChIMiC-project aims at creating a paradigm shift in the quality and speed with which the detailed chemical composition of vapors, mixtures, cells and tissues can be measured. Searching for a disease marking molecule in exhaled breath or a toxic waste product in drinking water literally corresponds to finding a needle in a haystack. To detect these molecules, they first need to be (partially) separated from thousands of other components. The better the separation, the more reliable and precise the measurement. Today, the identification problems in environmental and life sciences have become so complex that the separation capacity needed to solve them can no longer be achieved using a single separation mode but requires a multi-dimensional separation space. To resolve this, a concerted effort is proposed, providing both hardware and software solutions to enlarge this separation space as well as to help the chemical analysts make maximal use of it. For the latter end, a powerful decision-supporting software tool will be developed to undo with the current practice based on trial-and-error or analysts' intuition. This new rational and algorithmic approach to chemical measurement design can be expected to boost the development of new drugs, and provide more refined clinical diagnostic tests and safer food. The project consortium consists of 5 well-connected research groups with a strong international recognition in separation science, computer-modelling and microfluidics.\",\n",
       "  ['Chiral resolution of solutes occurring in mixtures of unrelated species is of relevance in life sciences and in pharmaceutical analysis. While this is conceptually achievable by comprehensive two-dimensional liquid chromatography (LC × LC), few approaches exist whereby the second dimension comprises the chiral separation. The latter is preferable in combination with a conventional reversed phase type of separation in the first dimension as it offers an extension of a conventional achiral analysis. The implementation of such rapid chiral analyses in the second dimension was, thus far, limited by the challenging transfer of the first dimension mobile phase to the second dimension while still achieving chiral separation. In this study, the combination of temperature-responsive and reversed-phase chiral liquid chromatography is assessed in terms of enantioselective separation of a broad range of pharmaceutical compounds. Applying temperature-responsive liquid chromatography (TRLC) in the first dimension allows for analyses to be performed under purely aqueous conditions, which then allows for complete and more generic refocusing of (organic) solutes prior to the second dimension. This offers an enhanced ability to employ fast and broad compositional gradients over the chiral dimension, which broadens the applicability of the technique. In the proposed platform, seven chiral columns (superficially porous and fully porous columns (comprising both polysaccharide and macrocyclic antibiotic phases)) and four mobile phase gradients were screened on a pharmaceutical test mixture. The platform was shown to be able to offer the necessary resolving power for the molecules at hand and offers a new approach for chiral screening of mixtures of unrelated compounds.']),\n",
       " ('Fever is a cardinal but nonspecific feature of systemic inflammation. More than 200 disorders are characterized by uncontrolled systemic inflammation, making it challenging to manage individual patients. Disorders encompass different entities such as autoimmune diseases, autoinflammatory disorders, or hyperinflammatory conditions. Despite some clinical similarities, these disorders comprise distinct pathophysiology and management strategies. \\n Distinguishing these diseases from benign causes of fever is pivotal, as early diagnosis and treatment results in decreased morbidity, mortality and health care costs. In these disorders, targeted treatment with biologicals has expanded during the previous years. Nevertheless, multiple caveats persist, such as toxicity, strict reimbursement criteria, and expensive drug development. Consequently, it is crucial that biological treatment is directed to patients with best benefit and is monitored by relevant biomarkers. Currently, a specific and rapid test to stratify patients and monitor response is lacking. Notably, reimbursement criteria for biological therapy rely on clinical suspicion or a proven genetic diagnosis, but so far, discriminatory lab abnormalities are not implemented. \\n With the multi-centre Flemish Joint Effort for Biomarker pRofiling in Inflammatory Systemic diseases (FEBRIS), we will integrate cytokine profiles by multiplex immunoassay (MIA) into the diagnostic workup and management of pediatric and adult patients with inflammatory syndromes of unknown origins and/or with insufficient response to first line therapy. In parallel with recent literature, we characterized specific cytokine profiles differentiating different diseases, allowing to implement it as a generic test that copes with the wide variety of diseases in the target group. The validation of our assay in larger cohorts with proven or suspected inflammatory disorders will guide time-, resource- and cost-efficient workup and personalized management.',\n",
       "  ['Background: Infection with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is clinically diverse, and children have a low risk of developing severe coronavirus disease 2019 (COVID-19). However, children with chronic diseases have a potentially increased risk. Methods: We performed a prospective surveillance study with longitudinal serum SARS-CoV-2 anti-nucleocapsid antibody quantification and questionnaires in pediatric tertiary care patients during the first waves of the COVID-19 pandemic (November 2020-September 2021). The results were compared with those of healthy children and adults from the same geographic area. Results: We obtained 525 samples from 362 patients (M/F ratio of 1.3:1; median age of 11.1 years) comprising children with immune-suppressive or immunemodulating drugs (32.9%), inborn errors of immunity (23.5%), type 1 diabetes mellitus (15.2%), and rheumatic diseases (11.9%). A total of 51 (9.7%) samples were seropositive among 37/351 children (10.5%). Seropositivity increased from 5.8% in November-December 2020 to 21.6% in July-September 2021. Compared with adults, a longitudinal analysis revealed reduced seroprevalence but similar kinetics as in children from the same country. Demographic or social variables and disease characteristics did not correlate with seropositivity. Being obese and household contact with COVID-19-infected individuals significantly increased the odds of infection. The majority of seropositive patients had mild symptoms (21/37). One-third were asymptomatic and/or unaware of having COVID-19 (10/37). Four patients (4/37) needed hospitalization, with good clinical outcomes. Conclusions: Although harboring a chronic disease, we observed a low SARS-CoV-2 incidence in a cohort of pediatric tertiary care patients, comparable with healthy children during the first year of the pandemic. Infection was mostly associated with mild symptoms.'])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # read json\n",
    "# with open('FRIS_projects_pubs_abstracts.json') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# all_documents = []\n",
    "# for entry in data:\n",
    "#     entry = dict(entry)\n",
    "#     uuid = list(entry.keys())[0]\n",
    "#     if entry[uuid]['pub_abstracts']:\n",
    "#         all_documents.append((entry[uuid]['project_abstract'], entry[uuid]['pub_abstracts']))\n",
    "\n",
    "# split = int(len(all_documents)*0.8)\n",
    "# labeled_documents, documents = all_documents[:split], all_documents[split:]\n",
    "# labeled_documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The relevance of the study of Latin & Greek (L&G) in secondary education is being questioned more and more. Proponents have claimed that it greatly improves linguistic abilities and even cognition beyond the linguistic domain, e.g. analytical thinking. Numerous American scholars have set up field tests, but they focused on the influence of Latin on English vocabulary acquisition by native speakers. Moreover, these studies all lack a theoretical framework for the supposed effects of L&G, so that the premises underlying hypotheses remain implicit. As a result, the debate on the educational value of L&G is still predominantly based on intuitive and ideologically-driven arguments rather than on empirical research. Therefore we propose a longitudinal study on the effects of L&G on Flemish pupils’ language competence (near transfer) and cognition (far transfer). Latin & Greek differ from the other languages standardly offered in Flemish secondary education because they are no longer spoken, but also because they are more complex. As such, the common didactic approach pays much more attention to grammatical parsing. We will investigate whether this intensive training in grammatical analysis results in cognitive transfer, which will be dissociated from a priori pupil characteristics. In addition to the main study, the features of L&G learning suspected to cause transfer will be simulated in a short-term artificial grammar learning experiment.',\n",
       "  ['0602', '0501']),\n",
       " ('Viruses have evolved to hijack key cellular components of their natural host. The VirEOS project will analyze how medically relevant viruses such as human respiratory syncytial, hepatitis E, yellow fever, Zika or Kaposi sarcoma-associated herpes virus, interfere with RNA sensing and RNA homeostasis in their host cells. The impact of identified factors on the pathogenicity and on the immune responses will then be analyzed in vivo, using infectious models. Understanding how viruses manipulate cellular RNA should not only provide new targets to the development of antiviral drugs but also help to identify important cellular hubs in RNA physiology.',\n",
       "  ['0106', '0306', '0302', '0301']),\n",
       " ('This project offers the first comprehensive reconstruction and interpretation of receptions of ancient novels (1st-4th cent. AD) in (Greek, Arabic and western vernacular) secular narrative from Late Antiquity and the early Middle Ages. Novel Echoes follows up from the ERC Starting Grant project Novel Saints (on hagiography). It does so by taking ancient novelistic receptions towards entirely new, unexplored horizons. Our knowledge about the early history of the novel is incomplete. Receptions of ancient novels have been studied for periods from the 11th and 12th cent. onwards but not systematically examined for preceding eras –much to the detriment of the study of both narrative (then and later) and the history of fiction. This project pursues the hypothesis that different secular, narrative traditions in this period were impacted (directly or indirectly) by ancient novelistic influences of different kinds and adopted (and adapted) them to various degrees and purposes; and that, since the ancient novel is a genre defined by its own fictionality, its reception in later narrative impacts notions of truth and authentication in ways that other (often more authoritative) literary models (e.g. Homer and the Bible) do not. Novel Echoes strikes a balance between breath and depth by envisaging three objectives: \\n 1. the creation of a reference tool charting all types of novelistic influence in secular narrative from the 4th to the 12th cent.; \\n 2. the in-depth study of particular sets of texts and the analysis of their implicit conceptualizations of truth, authentication, fiction and narrative; \\n 3. the reconstruction of routes of transmission in both the West and the East. Given the project’ innovative focus, it will enhance our understanding of both the corpus texts and the early history of the novel; place the study of corpus texts on an improved methodological footing; and contribute to the theoretical study of the much-vexed question of how to conceptualize fiction.',\n",
       "  ['0603']),\n",
       " (\"The ChIMiC-project aims at creating a paradigm shift in the quality and speed with which the detailed chemical composition of vapors, mixtures, cells and tissues can be measured. Searching for a disease marking molecule in exhaled breath or a toxic waste product in drinking water literally corresponds to finding a needle in a haystack. To detect these molecules, they first need to be (partially) separated from thousands of other components. The better the separation, the more reliable and precise the measurement. Today, the identification problems in environmental and life sciences have become so complex that the separation capacity needed to solve them can no longer be achieved using a single separation mode but requires a multi-dimensional separation space. To resolve this, a concerted effort is proposed, providing both hardware and software solutions to enlarge this separation space as well as to help the chemical analysts make maximal use of it. For the latter end, a powerful decision-supporting software tool will be developed to undo with the current practice based on trial-and-error or analysts' intuition. This new rational and algorithmic approach to chemical measurement design can be expected to boost the development of new drugs, and provide more refined clinical diagnostic tests and safer food. The project consortium consists of 5 well-connected research groups with a strong international recognition in separation science, computer-modelling and microfluidics.\",\n",
       "  ['0299']),\n",
       " ('Fever is a cardinal but nonspecific feature of systemic inflammation. More than 200 disorders are characterized by uncontrolled systemic inflammation, making it challenging to manage individual patients. Disorders encompass different entities such as autoimmune diseases, autoinflammatory disorders, or hyperinflammatory conditions. Despite some clinical similarities, these disorders comprise distinct pathophysiology and management strategies. \\n Distinguishing these diseases from benign causes of fever is pivotal, as early diagnosis and treatment results in decreased morbidity, mortality and health care costs. In these disorders, targeted treatment with biologicals has expanded during the previous years. Nevertheless, multiple caveats persist, such as toxicity, strict reimbursement criteria, and expensive drug development. Consequently, it is crucial that biological treatment is directed to patients with best benefit and is monitored by relevant biomarkers. Currently, a specific and rapid test to stratify patients and monitor response is lacking. Notably, reimbursement criteria for biological therapy rely on clinical suspicion or a proven genetic diagnosis, but so far, discriminatory lab abnormalities are not implemented. \\n With the multi-centre Flemish Joint Effort for Biomarker pRofiling in Inflammatory Systemic diseases (FEBRIS), we will integrate cytokine profiles by multiplex immunoassay (MIA) into the diagnostic workup and management of pediatric and adult patients with inflammatory syndromes of unknown origins and/or with insufficient response to first line therapy. In parallel with recent literature, we characterized specific cytokine profiles differentiating different diseases, allowing to implement it as a generic test that copes with the wide variety of diseases in the target group. The validation of our assay in larger cohorts with proven or suspected inflammatory disorders will guide time-, resource- and cost-efficient workup and personalized management.',\n",
       "  ['0301'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read json\n",
    "with open('FRIS_projects_pubs_abstracts.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.read_csv('FRIS_projects_pubs.csv')\n",
    "\n",
    "all_documents = []\n",
    "for entry in data:\n",
    "    entry = dict(entry)\n",
    "    uuid = list(entry.keys())[0]\n",
    "    if entry[uuid]['pub_abstracts']:\n",
    "        temp = df[df['pro_id'] == uuid]['disciplines'].values[0]\n",
    "        temp = temp.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(',')\n",
    "        temp = [x.strip() for x in temp]\n",
    "        all_documents.append((entry[uuid]['project_abstract'], temp))\n",
    "\n",
    "split = int(len(all_documents)*0.8)\n",
    "labeled_documents, documents = all_documents[:split], all_documents[split:]\n",
    "labeled_documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llda_model \n",
      "Labeled-LDA Model:\n",
      "\tK = 42\n",
      "\tM = 460\n",
      "\tT = 20377\n",
      "\tWN = 132085\n",
      "\tLN = 836\n",
      "\talpha = 1.1904761904761905\n",
      "\teta = 0.001\n",
      "\tperplexity = 852.0891495398791\n",
      "\t\n",
      "Doc-Topic Matrix: \n",
      " [[0.04599183 0.17687513 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.24232707 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.2961165  ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "Topic-Term Matrix: \n",
      " [[9.00595023e-07 9.00595023e-07 9.00595023e-07 ... 9.00595023e-07\n",
      "  9.00595023e-07 9.00595023e-07]\n",
      " [1.23399356e-06 1.23399356e-06 1.23399356e-06 ... 1.23399356e-06\n",
      "  1.23399356e-06 1.23399356e-06]\n",
      " [4.20454789e-07 4.20454789e-07 4.20454789e-07 ... 4.20454789e-07\n",
      "  4.20454789e-07 4.20454789e-07]\n",
      " ...\n",
      " [9.14597618e-07 9.14597618e-07 9.14597618e-07 ... 9.14597618e-07\n",
      "  9.14597618e-07 9.14597618e-07]\n",
      " [9.55678498e-07 9.55678498e-07 9.55678498e-07 ... 9.55678498e-07\n",
      "  9.55678498e-07 9.55678498e-07]\n",
      " [1.84373600e-06 1.84373600e-06 1.84373600e-06 ... 1.84373600e-06\n",
      "  1.84373600e-06 1.84373600e-06]]\n"
     ]
    }
   ],
   "source": [
    "# load from disk\n",
    "llda_model = llda.LldaModel()\n",
    "llda_model.load_model_from_dir(\"LLDA/data_models/pro_abstracts_disciplines\", load_derivative_properties=False)\n",
    "print(\"llda_model\", llda_model)\n",
    "# print(\"Top-5 terms of topic 'virus': \", llda_model.top_terms_of_topic(\"virus\", 5, False))\n",
    "print(\"Doc-Topic Matrix: \\n\", llda_model.theta)\n",
    "print(\"Topic-Term Matrix: \\n\", llda_model.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 1291)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llda_model.theta.shape\n",
    "# 1 - pairwise_distances(llda_model.theta, metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeled-LDA Model:\n",
      "\tK = 42\n",
      "\tM = 460\n",
      "\tT = 20377\n",
      "\tWN = 132085\n",
      "\tLN = 836\n",
      "\talpha = 1.1904761904761905\n",
      "\teta = 0.001\n",
      "\tperplexity = 968.6534305207664\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "# llda_model = llda.LldaModel(labeled_documents=labeled_documents)\n",
    "# print(llda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration: 22, perplexity: 870.0354770072772\n",
      "gibbs sample count:  132085\n",
      "after iteration: 23, perplexity: 868.5819937848004\n",
      "gibbs sample count:  132085\n",
      "after iteration: 24, perplexity: 869.6295601174832\n",
      "gibbs sample count:  132085\n",
      "after iteration: 25, perplexity: 869.5437562486107\n",
      "gibbs sample count:  132085\n",
      "after iteration: 26, perplexity: 869.4989849936198\n",
      "gibbs sample count:  132085\n",
      "after iteration: 27, perplexity: 868.6138717233073\n",
      "gibbs sample count:  132085\n",
      "after iteration: 28, perplexity: 866.9250289282331\n",
      "gibbs sample count:  132085\n",
      "after iteration: 29, perplexity: 868.3025402312052\n",
      "gibbs sample count:  132085\n",
      "after iteration: 30, perplexity: 866.1864868845389\n",
      "gibbs sample count:  132085\n",
      "after iteration: 31, perplexity: 866.6590742357713\n",
      "gibbs sample count:  132085\n",
      "after iteration: 32, perplexity: 867.0894292052224\n",
      "gibbs sample count:  132085\n",
      "after iteration: 33, perplexity: 865.911075661152\n",
      "gibbs sample count:  132085\n",
      "after iteration: 34, perplexity: 866.3648719927676\n",
      "gibbs sample count:  132085\n",
      "after iteration: 35, perplexity: 865.0472588825357\n",
      "gibbs sample count:  132085\n",
      "after iteration: 36, perplexity: 864.2526496798646\n",
      "gibbs sample count:  132085\n",
      "after iteration: 37, perplexity: 863.0543365614944\n",
      "gibbs sample count:  132085\n",
      "after iteration: 38, perplexity: 863.1829030855952\n",
      "gibbs sample count:  132085\n",
      "after iteration: 39, perplexity: 863.2472909728438\n",
      "gibbs sample count:  132085\n",
      "after iteration: 40, perplexity: 862.8145375764801\n",
      "gibbs sample count:  132085\n",
      "after iteration: 41, perplexity: 861.9939414749404\n",
      "gibbs sample count:  132085\n",
      "after iteration: 42, perplexity: 861.9805783807902\n",
      "gibbs sample count:  132085\n",
      "after iteration: 43, perplexity: 862.579679220537\n",
      "gibbs sample count:  132085\n",
      "after iteration: 44, perplexity: 861.6424505972066\n",
      "gibbs sample count:  132085\n",
      "after iteration: 45, perplexity: 862.2884123120479\n",
      "gibbs sample count:  132085\n",
      "after iteration: 46, perplexity: 861.4604899602715\n",
      "gibbs sample count:  132085\n",
      "after iteration: 47, perplexity: 862.3825976395145\n",
      "gibbs sample count:  132085\n",
      "after iteration: 48, perplexity: 862.2482346937428\n",
      "gibbs sample count:  132085\n",
      "after iteration: 49, perplexity: 861.2432805619007\n",
      "gibbs sample count:  132085\n",
      "after iteration: 50, perplexity: 861.7053740875398\n",
      "gibbs sample count:  132085\n",
      "after iteration: 51, perplexity: 862.3760575162971\n",
      "gibbs sample count:  132085\n",
      "after iteration: 52, perplexity: 861.7833945040278\n",
      "gibbs sample count:  132085\n",
      "after iteration: 53, perplexity: 861.8498036079401\n",
      "gibbs sample count:  132085\n",
      "after iteration: 54, perplexity: 862.3171061781301\n",
      "gibbs sample count:  132085\n",
      "after iteration: 55, perplexity: 863.1957936806772\n",
      "gibbs sample count:  132085\n",
      "after iteration: 56, perplexity: 861.7834772340742\n",
      "gibbs sample count:  132085\n",
      "after iteration: 57, perplexity: 861.0778479480418\n",
      "gibbs sample count:  132085\n",
      "after iteration: 58, perplexity: 860.2458021282745\n",
      "gibbs sample count:  132085\n",
      "after iteration: 59, perplexity: 861.4190258206127\n",
      "gibbs sample count:  132085\n",
      "after iteration: 60, perplexity: 861.4332275760703\n",
      "gibbs sample count:  132085\n",
      "after iteration: 61, perplexity: 860.7962379523157\n",
      "gibbs sample count:  132085\n",
      "after iteration: 62, perplexity: 860.251196691264\n",
      "gibbs sample count:  132085\n",
      "after iteration: 63, perplexity: 859.9368373871579\n",
      "gibbs sample count:  132085\n",
      "after iteration: 64, perplexity: 860.7068823877794\n",
      "gibbs sample count:  132085\n",
      "after iteration: 65, perplexity: 860.2864682207523\n",
      "gibbs sample count:  132085\n",
      "after iteration: 66, perplexity: 859.3778534879849\n",
      "gibbs sample count:  132085\n",
      "after iteration: 67, perplexity: 860.6638082721869\n",
      "gibbs sample count:  132085\n",
      "after iteration: 68, perplexity: 861.0088347666751\n",
      "gibbs sample count:  132085\n",
      "after iteration: 69, perplexity: 859.7674377272266\n",
      "gibbs sample count:  132085\n",
      "after iteration: 70, perplexity: 860.6745425946002\n",
      "gibbs sample count:  132085\n",
      "after iteration: 71, perplexity: 860.0962419589038\n",
      "gibbs sample count:  132085\n",
      "after iteration: 72, perplexity: 859.6034835143804\n",
      "gibbs sample count:  132085\n",
      "after iteration: 73, perplexity: 859.0248052598464\n",
      "gibbs sample count:  132085\n",
      "after iteration: 74, perplexity: 858.9418956375091\n",
      "gibbs sample count:  132085\n",
      "after iteration: 75, perplexity: 858.5591918747577\n",
      "gibbs sample count:  132085\n",
      "after iteration: 76, perplexity: 857.3107095819508\n",
      "gibbs sample count:  132085\n",
      "after iteration: 77, perplexity: 856.6935932288392\n",
      "gibbs sample count:  132085\n",
      "after iteration: 78, perplexity: 856.64290889606\n",
      "gibbs sample count:  132085\n",
      "after iteration: 79, perplexity: 855.0134329744847\n",
      "gibbs sample count:  132085\n",
      "after iteration: 80, perplexity: 854.7349594783195\n",
      "gibbs sample count:  132085\n",
      "after iteration: 81, perplexity: 855.7425537406396\n",
      "gibbs sample count:  132085\n",
      "after iteration: 82, perplexity: 853.9852615025281\n",
      "gibbs sample count:  132085\n",
      "after iteration: 83, perplexity: 853.8276521967017\n",
      "gibbs sample count:  132085\n",
      "after iteration: 84, perplexity: 854.0398790798221\n",
      "gibbs sample count:  132085\n",
      "after iteration: 85, perplexity: 854.5574023373538\n",
      "gibbs sample count:  132085\n",
      "after iteration: 86, perplexity: 853.6221156459127\n",
      "gibbs sample count:  132085\n",
      "after iteration: 87, perplexity: 854.3952149406678\n",
      "gibbs sample count:  132085\n",
      "after iteration: 88, perplexity: 853.6143395232107\n",
      "gibbs sample count:  132085\n",
      "after iteration: 89, perplexity: 854.9612048927653\n",
      "gibbs sample count:  132085\n",
      "after iteration: 90, perplexity: 854.7881266224889\n",
      "gibbs sample count:  132085\n",
      "after iteration: 91, perplexity: 855.1712197663935\n",
      "gibbs sample count:  132085\n",
      "after iteration: 92, perplexity: 855.1843590615449\n",
      "gibbs sample count:  132085\n",
      "after iteration: 93, perplexity: 855.0275178282257\n",
      "gibbs sample count:  132085\n",
      "after iteration: 94, perplexity: 855.1578976801167\n",
      "gibbs sample count:  132085\n",
      "after iteration: 95, perplexity: 854.6846344065335\n",
      "gibbs sample count:  132085\n",
      "after iteration: 96, perplexity: 854.6338170826721\n",
      "gibbs sample count:  132085\n",
      "after iteration: 97, perplexity: 855.3753893443554\n",
      "gibbs sample count:  132085\n",
      "after iteration: 98, perplexity: 855.8295302331276\n",
      "gibbs sample count:  132085\n",
      "after iteration: 99, perplexity: 855.3765352092171\n",
      "gibbs sample count:  132085\n",
      "after iteration: 100, perplexity: 854.9813054211999\n",
      "gibbs sample count:  132085\n",
      "after iteration: 101, perplexity: 854.7095320870999\n",
      "gibbs sample count:  132085\n",
      "after iteration: 102, perplexity: 854.6609380216994\n",
      "gibbs sample count:  132085\n",
      "after iteration: 103, perplexity: 854.2962418899584\n",
      "gibbs sample count:  132085\n",
      "after iteration: 104, perplexity: 855.2237198382969\n",
      "gibbs sample count:  132085\n",
      "after iteration: 105, perplexity: 854.7454896654443\n",
      "gibbs sample count:  132085\n",
      "after iteration: 106, perplexity: 854.418370293253\n",
      "gibbs sample count:  132085\n",
      "after iteration: 107, perplexity: 854.1348327292873\n",
      "gibbs sample count:  132085\n",
      "after iteration: 108, perplexity: 854.0860179791521\n",
      "gibbs sample count:  132085\n",
      "after iteration: 109, perplexity: 853.3076958019725\n",
      "gibbs sample count:  132085\n",
      "after iteration: 110, perplexity: 853.0634926489274\n",
      "gibbs sample count:  132085\n",
      "after iteration: 111, perplexity: 852.1236173188854\n",
      "gibbs sample count:  132085\n",
      "after iteration: 112, perplexity: 852.6113830438003\n",
      "gibbs sample count:  132085\n",
      "after iteration: 113, perplexity: 851.5196163138114\n",
      "gibbs sample count:  132085\n",
      "after iteration: 114, perplexity: 852.2152434937651\n",
      "gibbs sample count:  132085\n",
      "after iteration: 115, perplexity: 851.8871800571401\n",
      "gibbs sample count:  132085\n",
      "after iteration: 116, perplexity: 853.0137014161255\n",
      "gibbs sample count:  132085\n",
      "after iteration: 117, perplexity: 852.4607398313195\n",
      "gibbs sample count:  132085\n",
      "after iteration: 118, perplexity: 852.4093562472225\n",
      "gibbs sample count:  132085\n",
      "after iteration: 119, perplexity: 854.6410219589006\n",
      "gibbs sample count:  132085\n",
      "after iteration: 120, perplexity: 853.2438792932943\n",
      "gibbs sample count:  132085\n",
      "after iteration: 121, perplexity: 852.4293642581756\n",
      "gibbs sample count:  132085\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "# llda_model.training(iteration=100, log=True)\n",
    "# while True:\n",
    "#     print(\"iteration %s sampling...\" % (llda_model.iteration + 1))\n",
    "#     llda_model.training(1)\n",
    "#     print(\"after iteration: %s, perplexity: %s\" % (llda_model.iteration, llda_model.perplexity()))\n",
    "#     print(\"delta beta: %s\" % llda_model.delta_beta)\n",
    "#     if llda_model.is_convergent(method=\"beta\", delta=0.01):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # update\n",
    "# print(\"before updating: \", llda_model)\n",
    "# update_labeled_documents = [(\"new example test example test example test example test\", [\"example\", \"test\"])]\n",
    "# llda_model.update(labeled_documents=update_labeled_documents)\n",
    "# print(\"after updating: \", llda_model)\n",
    "\n",
    "# # train again\n",
    "# # llda_model.training(iteration=10, log=True)\n",
    "# while True:\n",
    "#     print(\"iteration %s sampling...\" % (llda_model.iteration + 1))\n",
    "#     llda_model.training(1)\n",
    "#     print(\"after iteration: %s, perplexity: %s\" % (llda_model.iteration, llda_model.perplexity()))\n",
    "#     print(\"delta beta: %s\" % llda_model.delta_beta)\n",
    "#     if llda_model.is_convergent(method=\"beta\", delta=0.01):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IOF programme brings together expertise in (1) agricultural and food science, (2) sensor technologies and (3) statistics to help the agrifood industry through the fourth industrial revolution.  The main goal of the programme is to valorise the basic research in these disciplines together with relevant stakeholders.  Focus of the sensor-related work is nondestructive quality assessment using biophotonics.  The statistics work mainly focuses on advanced experimental design, multivariate statistics and statistical process control.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('0101', 0.012848932676518885),\n",
       " ('0102', 0.013711001642036128),\n",
       " ('0103', 0.018883415435139578),\n",
       " ('0104', 0.011986863711001643),\n",
       " ('0105', 0.01026272577996716),\n",
       " ('0106', 0.012848932676518885),\n",
       " ('0107', 0.019745484400656814),\n",
       " ('0199', 0.011124794745484402),\n",
       " ('0201', 0.011124794745484402),\n",
       " ('0202', 0.011124794745484402),\n",
       " ('0203', 0.020607553366174057),\n",
       " ('0204', 0.01026272577996716),\n",
       " ('0205', 0.012848932676518885),\n",
       " ('0206', 0.012848932676518885),\n",
       " ('0207', 0.011124794745484402),\n",
       " ('0208', 0.011124794745484402),\n",
       " ('0299', 0.01026272577996716),\n",
       " ('0301', 0.01026272577996716),\n",
       " ('0302', 0.011124794745484402),\n",
       " ('0303', 0.011124794745484402),\n",
       " ('0304', 0.01026272577996716),\n",
       " ('0305', 0.013711001642036128),\n",
       " ('0306', 0.011124794745484402),\n",
       " ('0399', 0.01026272577996716),\n",
       " ('0401', 0.015435139573070609),\n",
       " ('0402', 0.01026272577996716),\n",
       " ('0499', 0.01026272577996716),\n",
       " ('0501', 0.011986863711001643),\n",
       " ('0502', 0.02750410509031199),\n",
       " ('0503', 0.01026272577996716),\n",
       " ('0504', 0.013711001642036128),\n",
       " ('0505', 0.01026272577996716),\n",
       " ('0506', 0.011124794745484402),\n",
       " ('0507', 0.011124794745484402),\n",
       " ('0508', 0.01026272577996716),\n",
       " ('0599', 0.015435139573070609),\n",
       " ('0601', 0.016297208538587852),\n",
       " ('0602', 0.012848932676518885),\n",
       " ('0603', 0.011124794745484402),\n",
       " ('0604', 0.013711001642036128),\n",
       " ('0699', 0.03957307060755337),\n",
       " ('common_topic', 0.4481937602627258)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference\n",
    "# note: the result topics may be different for difference training, because gibbs sampling is a random algorithm\n",
    "document = documents[0][0]\n",
    "print(document)\n",
    "\n",
    "topics = llda_model.inference(document=document, iteration=100, times=10)\n",
    "# print sorted topics\n",
    "sorted(topics, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01112479 0.01198686 0.013711   ... 0.02233169 0.01026273 0.43440066]\n",
      " [0.00594689 0.00825104 0.00548607 ... 0.00871187 0.00686855 0.52898837]\n",
      " [0.0070028  0.0170028  0.0070028  ... 0.00817927 0.00759104 0.55759104]\n",
      " ...\n",
      " [0.00954914 0.00715552 0.00396403 ... 0.00742148 0.01752786 0.69279382]\n",
      " [0.00248189 0.00385346 0.00372877 ... 0.00323002 0.00185845 0.69337965]\n",
      " [0.00668498 0.00572344 0.00860806 ... 0.01245421 0.00812729 0.5658196 ]]\n"
     ]
    }
   ],
   "source": [
    "topic_matrix = np.zeros((len(documents), llda_model.K))\n",
    "for i, (document, _) in enumerate(documents):\n",
    "    topics = llda_model.inference(document=document, iteration=100, times=10)\n",
    "    sorted_topics = sorted(topics, key=lambda x: x[0])\n",
    "    for j, (topic, prob) in enumerate(sorted_topics):\n",
    "        topic_matrix[i, j] = prob\n",
    "print(topic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity on test data: 1115.2104641106398\n",
      "perplexity on training data: 852.0902032423336\n"
     ]
    }
   ],
   "source": [
    "# perplexity\n",
    "# calculate perplexity on test data\n",
    "perplexity = llda_model.perplexity(documents=map(lambda x: x[0], documents),\n",
    "                                   iteration=30,\n",
    "                                   times=10)\n",
    "print(\"perplexity on test data: %s\" % perplexity)\n",
    "# calculate perplexity on training data\n",
    "print(\"perplexity on training data: %s\" % llda_model.perplexity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "save_model_dir = \"LLDA/data_models/pro_abstracts_disciplines\"\n",
    "# llda_model.save_model_to_dir(save_model_dir, save_derivative_properties=True)\n",
    "llda_model.save_model_to_dir(save_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
